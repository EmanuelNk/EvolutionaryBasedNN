# -*- coding: utf-8 -*-
"""NN_EA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H66qo-2FwW8hTlXgsdHwE289rJXKlEwK
"""

import os
from standardize import generate_input_files
 
import numpy
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
from typing import List
import pandas as pd
from pandas import DataFrame
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Model
from sklearn.model_selection import train_test_split
from sklearn.metrics import fbeta_score, accuracy_score
import numpy as np
import random
import argparse

opt = tf.keras.optimizers.Adam(learning_rate=0.001)

def fitness(model: Model, X_train: DataFrame, y_train: DataFrame):
    predictions = model.predict(X_train, batch_size=None, verbose=0, steps=None,
                                callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=True)
    predictions = tf.round(predictions)
    return fbeta_score(y_true=y_train, y_pred=predictions, beta=0.25)
    # return accuracy_score(y_true=y_train, y_pred=predictions)

def mutate(model: Model, X_train: DataFrame, y_train: DataFrame) -> List[Model]:
    x_splt = np.array_split(X_train, 4)
    y_splt = np.array_split(y_train, 4)
 
    new_population = [keras.models.clone_model(model) for _ in range(4)]
 
    for x, y, chromosome in zip(x_splt, y_splt, new_population):
        chromosome.compile(optimizer=opt,
                           loss='binary_crossentropy',
                           metrics=['accuracy'])
        chromosome.set_weights(model.get_weights())
        chromosome.fit(x, y, epochs=1, batch_size=1)
    return new_population

def init_population():
    population = [keras.Sequential([
        keras.layers.Flatten(input_shape=(16,)),
        keras.layers.Dense(20, activation=tf.nn.relu),
        keras.layers.Dense(20, activation=tf.nn.relu),
        keras.layers.Dense(1, activation=tf.nn.sigmoid)
    ]) for i in range(5)]
 
    for model in population:
        model.compile(optimizer=opt,
                      loss='binary_crossentropy',
                      metrics=['accuracy'])
    return population

def run(train_path: str, validate_path: str, test_path: str, load_model: bool, threshold=0.67, predict_only=False):
    if not os.path.exists('max_model'):
        os.mkdir('max_model')
    random.seed(42)
    tf.random.set_seed(42)
    properties = [f'IN{i}' for i in range(16)]

    test_df = pd.read_csv(test_path, names=['label'] + properties)
    X_test = test_df[properties]

    if predict_only:
        model = tf.keras.models.load_model('max_model')
        predictions = model.predict(X_test, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=True)
        return

    train_df = pd.read_csv(train_path,
                           names=['label'] + properties).head(400000)
    X_train = train_df[properties]
    y_train = train_df['label']
 
    validate_df = pd.read_csv(validate_path, names=['label'] + properties)
    X_validate = validate_df[properties]
    y_validate = validate_df['label']
  
    if load_model:
        population = [tf.keras.models.load_model(
            'max_model') for _ in range(5)]
        validate_fitness = fitness(population[0], X_validate, y_validate)
        print('F0.25 on validate:', validate_fitness)
        predictions = population[0].predict(X_test, batch_size=None, verbose=0, steps=None,
                                callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=True)
        predictions = tf.round(predictions)
        numpy.savetxt("out.txt", predictions, fmt='%i', delimiter=",")
 
 
    else:
        population = init_population()
 
    for i in range(1000):
        max_fitness = 0
        max_model = None
        X_fitness, X_mutate, y_fitness, y_mutate = train_test_split(
            X_train, y_train, test_size=0.5, random_state=random.randint(0, 100))
        for model in population:
            model_fitness = fitness(model, X_fitness, y_fitness)
            print('fitness', model_fitness)     
            if model_fitness > max_fitness:
                max_fitness = model_fitness
                max_model = model

        validate_fitness = fitness(max_model, X_validate, y_validate)
        print('F0.25 on validate:', validate_fitness)  
        if validate_fitness >= threshold:
            predictions = population[0].predict(X_test, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=True)
            predictions = tf.round(predictions)
            numpy.savetxt("out.txt", predictions, fmt='%i', delimiter=",")
            max_model.save('max_model')
            return
 
        max_model.save('max_model')
        population = mutate(max_model, X_mutate, y_mutate) + [max_model]

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--train", default='train.csv', help="train.csv file path")
    parser.add_argument("--validate", default='validate.csv', help="validate.csv file path")
    parser.add_argument("--test", default='test.csv', help="test.csv file path")
    parser.add_argument("--do_not_preprocess", help="do not preprocess the input files",
                    action="store_true")
    parser.add_argument("-l", "--load_model", help="load saved model",
                    action="store_true")
    parser.add_argument("-p", "--predict_only", help="make a prediction on the test file and exit (must have saved model)",
                    action="store_true")
    parser.add_argument("-t", "--threshold", type=float, default=0.67, help="fitness threshold on predict.csv to stop the running")

    args = parser.parse_args()
    if not args.do_not_preprocess:
        train, validate, test = generate_input_files(args.train), generate_input_files(args.validate), generate_input_files(args.test)
    else:
        train, validate, test = args.train, args.validate, args.test

    run(train, validate, test, args.load_model, args.threshold, args.predict_only)
